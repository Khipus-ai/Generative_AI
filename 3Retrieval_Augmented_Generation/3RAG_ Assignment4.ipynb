{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khipus.ai\n",
    "## Retrieval Augmented Generation\n",
    "### RAG Assignment 4\n",
    "### LangChain + Azure OpenAI + Pinecone\n",
    "### Name: (You Name here)\n",
    "<span>© Copyright Notice 2025, Khipus.ai - All Rights Reserved.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Note: This notebook requires Python 3.11. You can download from here https://www.python.org/ftp/python/3.11.0/python-3.11.0rc2-amd64.exe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation (RAG) for question answering using PDF documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Dependencies \n",
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Read Pinecone and Azure OpenAI Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read Pinecone and Azure OpenAI Environment Variables\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"YOUR_AZURE_OPENAI_KEY\" #key from the Azure OpenAI resource\n",
    "os.environ[\"AZURE_OPENAI_API_BASE\"] = \"YOUR_AZURE_OPENAI_ENDPOINT\"#https://azure-openai-<your-resource-name>.openai.azure.com/\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT\"] = \"text-embedding-ada-002\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"YOUR_PINECONE_API_KEY\" #key from the Pinecone resource\n",
    "\n",
    "PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "AZURE_OPENAI_API_KEY = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_OPENAI_API_BASE = os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "AZURE_OPENAI_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    "AZURE_OPENAI_API_VERSION = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n",
    "\n",
    "openai.api_key = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "openai.api_base = os.environ[\"AZURE_OPENAI_API_BASE\"]\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Load your PDF and split into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you will load your employee_handbook.pdf file and split it into smaller chunks for processing. The document will be divided into manageable segments to create embeddings that can be stored in our vector database.\n",
    "\n",
    "To use your own employee handbook:\n",
    "1. Place your PDF file in the `./docs/` directory\n",
    "2. Update the `pdf_path` variable in the next cell to point to your file\n",
    "3. The handbook will be automatically chunked into segments of 1000 characters with 100 character overlap\n",
    "4. These chunks will later be embedded and stored in Pinecone for retrieval\n",
    "\n",
    "This approach allows you to query information from the employee handbook using natural language questions in the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 document(s) and split into 23 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Step 3: Load your PDF and split into chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Azure OpenAI embeddings object using LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize the Azure OpenAI embeddings object using LangChain.\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_key=openai.api_key,\n",
    "    azure_endpoint=openai.api_base,  \n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Connect to Pinecone Client and create Index if doesnt exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available indexes: ['langchain-demo2']\n",
      "Created index: assignment4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace these values as needed\n",
    "api_key = PINECONE_API_KEY\n",
    "index_name = \"assignment4\"\n",
    "\n",
    "# Create an instance of the Pinecone class using the new API\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "# List indexes to check connectivity\n",
    "print(\"Available indexes:\", pc.list_indexes().names())\n",
    "\n",
    "# Create (or connect to) your Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,  \n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"  \n",
    "        )\n",
    "    )\n",
    "    print(f\"Created index: {index_name}\")\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Create and store embeddings using the PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings have been successfully stored in Pinecone!\n"
     ]
    }
   ],
   "source": [
    "# Step 6 Create and store embeddings using the PineconeVectorStore AND called it \"vectorstore\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Perform a similarity search and retrieve the most relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crist\\AppData\\Local\\Temp\\ipykernel_47272\\1460044448.py:4: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import AzureChatOpenAI``.\n",
      "  llm = AzureChatOpenAI(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv2\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://khipusaigpt0566189501.services.ai.azure.com to https://khipusaigpt0566189501.services.ai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv2\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:181: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\crist\\OneDrive\\Documentos\\GitHub\\Generative_AI\\.venv2\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:189: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://khipusaigpt0566189501.services.ai.azure.com to https://khipusaigpt0566189501.services.ai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Perform a similarity search and retrieve the most relevant documents\n",
    "\n",
    "# Initialize the language model using Azure Chat OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0,\n",
    "    openai_api_base=AZURE_OPENAI_API_BASE,\n",
    "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "    openai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=os.environ.get(\"AZURE_OPENAI_GPT4_MODEL_NAME\", \"gpt-4o\")\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Load the QA chain and run the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Load the QA chain and run the query\n",
    "# Load the QA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# Define your query\n",
    "query = \"What is Contoso Electronics’ mission statement?\"\n",
    "#How often are employee performance reviews conducted?\n",
    "\n",
    "# Retrieve similar documents from the vector store (removed include_metadata)\n",
    "# Answer here\n",
    "\n",
    "# Get the answer from the t\n",
    "# Answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Test Your RAG System\n",
    "\n",
    "Test your Retrieval Augmented Generation system using the following questions. For each question, replace the `query` variable in the previous cell and run it to see how well your system retrieves and answers based on the document content:\n",
    "\n",
    "1. How can employees report unethical or illegal conduct confidentially?\n",
    "2. How often are employee performance reviews conducted?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#  How can employees report unethical or illegal conduct confidentially?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youe code here\n",
    "# How often are employee performance reviews conducted?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
